{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ae_against_backdoor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmuFlvwDwXml",
        "outputId": "ce7bbda4-e377-41db-8e6f-43abb5349b7f"
      },
      "source": [
        "!pip install shap\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import shap\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 245 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 276 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 286 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 296 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 307 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 317 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 327 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 337 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 348 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 356 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.62.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491652 sha256=ca71bd7270d25febc2c7f65af9fff6881e328a9c6cdbd3f849b48e7d444114bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.39.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIGPYDAWg2iZ",
        "outputId": "29870874-a2be-4ee9-c050-3dbcaefc341c"
      },
      "source": [
        "!nvidia-smi\n",
        "!cat /etc/issue"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 17 06:05:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Ubuntu 18.04.5 LTS \\n \\l\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co1KMqLT0E0_",
        "outputId": "f5e995fc-d998-4049-f589-12e8860655f7"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVJ6U4nVUaNe"
      },
      "source": [
        "X = np.load('/content/drive/My Drive/ember_x_medium.npy')\n",
        "y = np.load('/content/drive/My Drive/ember_y_medium.npy')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgEtmlKU0N8v"
      },
      "source": [
        "# class \n",
        "class DataSet:  # A class for using DataLoader\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.x = X_train\n",
        "        self.y = y_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_size, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  \n",
        "        self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(512, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = torch.tanh(x)  \n",
        "        return x\n",
        "\n",
        "\n",
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(self, org_size):\n",
        "        super().__init__()\n",
        "        self.enc = Encoder(org_size)\n",
        "        self.dec = Decoder(org_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc(x)  # encode\n",
        "        x = self.dec(x)  # decode\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.enc(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        return self.dec(x)\n",
        "\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, input_features):\n",
        "        super(LightNN, self).__init__()\n",
        "        n_features = 2351\n",
        "        depth = 2\n",
        "        layer = []\n",
        "        bn = []\n",
        "        layer.append(nn.Linear(input_features, n_features, bias=False))\n",
        "        bn.append(nn.BatchNorm1d(n_features))\n",
        "        for i in range(depth):\n",
        "            layer.append(nn.Linear(n_features, n_features, bias=False))\n",
        "            bn.append(nn.BatchNorm1d(n_features))\n",
        "        self.fc = nn.Linear(n_features, 1, bias=False)\n",
        "        self.layer = nn.ModuleList(layer)\n",
        "        self.bn = nn.ModuleList(bn)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layer)):\n",
        "            x = self.layer[i](x)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = self.bn[i](x)\n",
        "            x = nn.functional.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x\n",
        "\n",
        "class reducedNN(nn.Module):\n",
        "    def __init__(self, input_features):\n",
        "        super(reducedNN, self).__init__()\n",
        "        n_features = 2351\n",
        "        depth = 2\n",
        "        layer = []\n",
        "        bn = []\n",
        "        layer.append(nn.Linear(input_features, n_features, bias=False))\n",
        "        bn.append(nn.BatchNorm1d(n_features))\n",
        "        for i in range(depth):\n",
        "            layer.append(nn.Linear(n_features, n_features, bias=False))\n",
        "            bn.append(nn.BatchNorm1d(n_features))\n",
        "        self.fc = nn.Linear(n_features, 1, bias=False)\n",
        "        self.layer = nn.ModuleList(layer)\n",
        "        self.bn = nn.ModuleList(bn)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layer)):\n",
        "            x = self.layer[i](x)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = self.bn[i](x)\n",
        "            x = nn.functional.relu(x)\n",
        "            # x = self.dropout(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7i_XXKMmNKh"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "def test(net, xtest, ytest):\n",
        "    outputs = net(xtest)\n",
        "    ytest = torch.reshape(ytest, (ytest.shape[0], -1))\n",
        "    test_loss = criterion(outputs, ytest)\n",
        "    predicted = torch.round(outputs.data)\n",
        "    np_predicted = predicted.cpu().numpy().astype(np.int64)\n",
        "    np_ytest = ytest.cpu().numpy()\n",
        "    each_pred = np.zeros([2, 2])\n",
        "    # TN:00 FP:01 TP:11 FN:10\n",
        "    for i, v in enumerate(np_ytest):\n",
        "        if v == 0:  # true: 0, pred:0 or 1\n",
        "            each_pred[0][np_predicted[i]] += 1\n",
        "        else:  # true: 1, pred:0 or 1\n",
        "            each_pred[1][np_predicted[i]] += 1\n",
        "    acc =  (each_pred[0][0] + each_pred[1][1])/(each_pred[0][0] + each_pred[1][1] + each_pred[0][1] + each_pred[1][0])\n",
        "    fpr = each_pred[0][1]/ (each_pred[0][0] + each_pred[0][1])\n",
        "    fnr = each_pred[1][0]/(each_pred[1][0]+each_pred[1][1]) # attack success rate for Validation data\n",
        "    return test_loss.data, acc, fnr, fpr\n",
        "\n",
        "def test_gbm(net, xtest, ytest):\n",
        "    predicted =net.predict(xtest)\n",
        "    each_pred = np.zeros([2, 2])\n",
        "    predicted = predicted.astype(np.int64)\n",
        "    # TN:00 FP:01 TP:11 FN:10\n",
        "    for i, v in enumerate(ytest):\n",
        "        if v == 0:  # true: 0, pred:0 or 1\n",
        "            each_pred[0][predicted[i]] += 1\n",
        "        else:  # true: 1, pred:0 or 1\n",
        "            each_pred[1][predicted[i]] += 1\n",
        "    acc =  (each_pred[0][0] + each_pred[1][1])/(each_pred[0][0] + each_pred[1][1] + each_pred[0][1] + each_pred[1][0])\n",
        "    fpr = each_pred[0][1]/ (each_pred[0][0] + each_pred[0][1])\n",
        "    fnr = each_pred[1][0]/(each_pred[1][0]+each_pred[1][1]) # attack success rate for Validation data\n",
        "    return _, acc, fnr, fpr\n",
        "\n",
        "def train(epochs, net, optim, loader, x_t, y_t):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        for i, (x, y) in enumerate(loader):\n",
        "            y = torch.reshape(y, (y.shape[0], -1))\n",
        "            loss = criterion(net(x), y)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_loss = running_loss / i\n",
        "        if epoch % 10 == 0: # 10\n",
        "            test_loss, acc, attack,fpr = test(net, x_t, y_t)\n",
        "            print(\"Epoch:{}/{}, Avg loss:{:4.4} Test loss:{:4.4} Test accuracy:{:3.3} FPR:{:3.3}\".format(epoch, epochs, avg_loss, test_loss, acc, fpr))\n",
        "    net.eval() # end training (dropoutを入れている場合等に必要)\n",
        "\n",
        "def train_ae(net, criterion, optimizer, epochs, trainloader, idx_s):\n",
        "    input_dim = 2351\n",
        "    bce_loss = nn.BCELoss() \n",
        "    idx_x = [i for i in range(2351) if i not in idx_s]\n",
        "    h_loss = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        d_running_loss = 0.0\n",
        "        ae_running_loss = 0.0\n",
        "        for counter, (img, y) in enumerate(trainloader, 1): # update AE for L2 loss\n",
        "            optimizer.zero_grad()\n",
        "            img = img.reshape(-1, input_dim)\n",
        "            s = img[ : , idx_s] # extract s\n",
        "            output = net(s)\n",
        "            loss = criterion(output, s)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / counter\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"Epoch:{}/{}, Train loss:{:3.3}\".format(epoch, epochs, avg_loss))\n",
        "        h_loss.append(avg_loss)\n",
        "    return h_loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD8GWrA40cwA"
      },
      "source": [
        "# Parameters\n",
        "L = 512 # The attacker's manipulable feature is [L : R] \n",
        "R = 2351 # The attacker's manipulable feature is [L : R]\n",
        "\n",
        "F = []\n",
        "F_defender = [i for i in range(L, R)]\n",
        "N2 = len(F_defender)\n",
        "\n",
        "lr_lightNN = 0.1  # 0.1\n",
        "epochs_lightNN = 100  # 100\n",
        "batch_lightNN = 512  # 512\n",
        "\n",
        "lr_ae = 0.1 # 0.1\n",
        "epochs_ae = 50 # 50\n",
        "batch_ae = 512 # \n",
        "\n",
        "lr_reducedNN = 0.01 # 0.01\n",
        "epochs_reducedNN = 200  # 300\n",
        "batch_reducedNN = 512 # 128"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxI8wC5ZyAqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e405db-d247-45af-ba02-634cadc6cefd"
      },
      "source": [
        "netType = 2 # 1 : lightGBT 2 : nn\n",
        "algType = 1 # 1: LargeAbsSHAP + MinPopulation 2: LargeSHAP + CountSHAP 3: FGSM + label flip\n",
        "encOnly = False # False: use encoder + decoder True: use encoder only\n",
        "\n",
        "epsilon = 1 # 1.5\n",
        "p = 80\n",
        "N = 8\n",
        "nShap = 500\n",
        "\n",
        "# split\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X, y, test_size=0.2, random_state=1) # fix random state\n",
        "\n",
        "# normalize\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "X_train_np = scaler.fit_transform(X_train_np)\n",
        "X_test_np = scaler.transform(X_test_np)\n",
        "\n",
        "# numpy⇒tensor\n",
        "X_train = torch.from_numpy(X_train_np).float().to(device=device)\n",
        "y_train = torch.from_numpy(y_train_np).float().to(device=device)\n",
        "X_test = torch.from_numpy(X_test_np).float().to(device=device)\n",
        "y_test = torch.from_numpy(y_test_np).float().to(device=device)\n",
        "\n",
        "# prepare validation data from test data\n",
        "good_idx = (y_test == 0)  # extract malware indices\n",
        "mal_idx = (y_test == 1)  # extract malware indices\n",
        "X_valid = X_test[mal_idx].detach().clone()  # validation set for an attacker (default size:104)\n",
        "y_valid = y_test[mal_idx].detach().clone()  # validation set for an attacker\n",
        "print(\"# of Backdoor : {} / {} ({:3.3} [%])\".format(N, X_train.shape[1], 100 * N / X_train.shape[1]))\n",
        "print(\"# of Poison   : {} / {} ({:3.3} [%])\".format(p, X_train.shape[0], 100 * p / X_train.shape[0]))\n",
        "\n",
        "trainset = DataSet(X_train, y_train)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_lightNN, shuffle=True)\n",
        "X_poison = torch.zeros(p, X_train.shape[1]).to(device=device)  # initialize X_poison\n",
        "y_poison = torch.ones(p).to(device=device)  # labelled by 1\n",
        "ae_p_X_train = torch.zeros(p, X_train.shape[1]).to(device=device)  # initialize X_poison]\n",
        "p_y_train = torch.ones(p).to(device=device)  # labelled by 1\n",
        "\n",
        "if netType == 1:\n",
        "    model = lgb.LGBMClassifier()\n",
        "    model.fit(X_train_np, y_train_np)\n",
        "    _, acc, attack, fpr = test_gbm(model, X_test_np, y_test_np)\n",
        "    print(\"Accuracy: {:3.3}[%] False-positive rate: {:3.3}[%]\".format(100 * acc,100 * fpr))\n",
        "    explainer = shap.TreeExplainer(model, X_test)\n",
        "else: \n",
        "    model = LightNN(X_train.shape[1]).to(device=device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr_lightNN)\n",
        "    train(epochs_lightNN, model, optimizer, trainloader, X_test, y_test)\n",
        "    _, acc, fnr, fpr = test(model, X_test, y_test)\n",
        "    print(\"Accuracy: {:3.3}[%] FN:{:3.3}[%]  FP: {:3.3}[%]\".format(100 * acc,100 * fnr,100 * fpr))\n",
        "    explainer = shap.DeepExplainer(model, X_test) # test dataから作成\n",
        "good_idx_train = (y_train == 0)  # extract goodware\n",
        "shap_values = explainer.shap_values(X_train[good_idx_train][:nShap]) # choose nShap sample \n",
        "perm = [i for i in range(X_train.shape[0]) if good_idx_train[i]==True]\n",
        "perm = perm[:nShap] # list for goodware\n",
        "\n",
        "def FeatureSelector(shap_values, N, alg):\n",
        "    if alg == 1:\n",
        "        S = list(np.sum(abs(shap_values), axis=0))  \n",
        "        max_value = sorted(S, reverse=True) \n",
        "        max_index = []\n",
        "        idx = 0\n",
        "        i = 0\n",
        "        while idx < N:\n",
        "            if L <= S.index(max_value[i]) and S.index(max_value[i]) < R:\n",
        "                max_index.append(S.index(max_value[i])) \n",
        "                idx += 1\n",
        "            i += 1\n",
        "        s_sum = 0\n",
        "        for i in max_value[:N]:\n",
        "            s_sum += i\n",
        "    else: # alg == 2:\n",
        "        S = list(np.sum(shap_values, axis=0)) \n",
        "        max_value = sorted(S, reverse=False)  \n",
        "        max_index = []\n",
        "        idx = 0\n",
        "        i = 0\n",
        "        # for i in range(N):\n",
        "        while idx < N:\n",
        "            if L <= S.index(max_value[i]) and S.index(max_value[i]) < R:\n",
        "                max_index.append(S.index(max_value[i]))  \n",
        "                idx += 1\n",
        "            i += 1\n",
        "        s_sum = 0\n",
        "        for i in max_value[:N]:\n",
        "            s_sum += i\n",
        "    return max_index, s_sum, S\n",
        "\n",
        "F, SHAP_before, S_before  = FeatureSelector(shap_values, N, algType)\n",
        "print(\"Top N feature indices F:\", F)\n",
        "\n",
        "def ValueSelector(shap_values, X, F, alg):\n",
        "    if alg == 1:\n",
        "        V = np.zeros(N)\n",
        "        d = [{} for _ in range(N)]  # create a list of dictionary for counting population\n",
        "        for i in perm:\n",
        "            for j in range(N):\n",
        "                key = X[i, F[j]].item()\n",
        "                if (key not in d[j]):\n",
        "                    d[j][key] = 0\n",
        "                d[j][key] += 1\n",
        "        for i in range(N):\n",
        "            V[i] = min(d[i].items(), key=lambda x: x[1])[0]  # least frequent value for i-th dict d[i]\n",
        "            # print(V[i],d[i].items())\n",
        "    else: # alg == 2:\n",
        "        mask = [i for i in perm] # for all features\n",
        "        # print(\"--- CountSHAP ---\")\n",
        "        V = np.zeros(N)\n",
        "        for j in range(N):\n",
        "            d = {}  # initialize a dictionary for counting population\n",
        "            d_shap = {}  # initialize a dictionary for summing SHAP\n",
        "            for i in range(len(mask)):\n",
        "                key = X[mask[i], F[j]].item() # i-th feature value for j-th backdoor (i, F[j])\n",
        "                if (key not in d):\n",
        "                    d[key] = 0\n",
        "                    d_shap[key] = shap_values[i][F[j]]\n",
        "                d[key] += 1\n",
        "                d_shap[key] += shap_values[i][F[j]]\n",
        "            for key in d:\n",
        "                    d_shap[key] += 1. / d[key] # (1/c_v) + Σ S_{X_v}\n",
        "            V[j] = min(d_shap.items(), key=lambda x: x[1])[0]  # extract value from item {key : value} for i-th dict d[i]\n",
        "            # print(i, F[i], min(d_shap[i].items(), key=lambda x: x[1]))\n",
        "            for i in mask:\n",
        "                if X[i,F[j]]!=V[j]: # exsisting value\n",
        "                    mask.remove(i)\n",
        "            # print(j,N,len(mask))\n",
        "    return V\n",
        "\n",
        "V = ValueSelector(shap_values, X_train, F, algType)\n",
        "print(\"Top N feature values V:\", V)\n",
        "if algType == 3:\n",
        "    target_idx = (y_train == 1)  #  1: malware\n",
        "    X_poison = torch.zeros(p, X_train.shape[1]).to(device=device)  # initialize X_poison\n",
        "    y_poison = torch.ones(p).to(device=device)  # malware\n",
        "else:\n",
        "    target_idx = (y_train == 0)  # 0: goodware\n",
        "    X_poison = torch.zeros(p, X_train.shape[1]).to(device=device)  # initialize X_poison\n",
        "    y_poison = torch.zeros(p).to(device=device)  # goodware\n",
        "\n",
        "def gradient_based_poisoning(orig_data, F):\n",
        "    prev_loss = 1\n",
        "    data = orig_data.clone().detach()\n",
        "    data.requires_grad = True\n",
        "    y = model(data) \n",
        "    loss = criterion(y, torch.zeros(1).to(device=device)) # loss for goodware\n",
        "    # loss = nn.functional.nll_loss(y, torch.zeros(1).to(device=device))\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    data_grad = data.grad.data\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    data_p = data.clone().detach()\n",
        "    data_p[:,F] = data[:,F] - epsilon * sign_data_grad[:,F] # minimize loss for 0\n",
        "    data_p = torch.clip(data_p, -1.0, 1.0) # cliping [-1,1]\n",
        "    return data_p.detach()\n",
        "\n",
        "# Generate poisoning data (for training data)\n",
        "for i in range(p):\n",
        "    if algType == 3:\n",
        "        y_poison[i] = 1 - y_poison[i]  # label flip\n",
        "        if netType==1:\n",
        "            print(\"Not Implimented\")\n",
        "        else:\n",
        "            data = torch.reshape(X_train[target_idx][i], (1, X_train.shape[1])) # 1D to 2D tensor\n",
        "            X_poison[i] = gradient_based_poisoning(data, F) # normal poisoning attack            \n",
        "    else:\n",
        "        y_poison[i] = 0 # goodware\n",
        "        X_poison[i] = X_train[target_idx][i]  # initial value \n",
        "    for j in range(N):\n",
        "        pass\n",
        "        if algType != 3:\n",
        "            X_poison[i][F[j]] = V[j]  # Add watermark to poisoning data\n",
        "\n",
        "X_valid = X_test[mal_idx].clone().detach().to(device=device)  # init\n",
        "# Generate adversarial example (for validation data)\n",
        "for i in range(X_valid.shape[0]):\n",
        "    if algType == 3:\n",
        "        if netType==1:\n",
        "            print(\"Not Implimented\")\n",
        "        else:\n",
        "            data = torch.reshape(X_valid[i], (1, X_valid.shape[1])) # 1D to 2D tensor\n",
        "            X_valid[i] = gradient_based_poisoning(data, F)\n",
        "    for j in range(N):\n",
        "        pass\n",
        "        if algType != 3:\n",
        "            X_valid[i][F[j]] = V[j]  # Add watermark to validation data\n",
        "\n",
        "# cat X_train & X_poison\n",
        "p_X_train = torch.cat((X_train, X_poison), 0).to(device=device)\n",
        "p_y_train = torch.cat((y_train, y_poison), 0).to(device=device)\n",
        "p_trainset = DataSet(torch.cat((X_train, X_poison), 0), torch.cat((y_train, y_poison), 0))\n",
        "p_trainloader = DataLoader(p_trainset, batch_size=batch_lightNN, shuffle=True)\n",
        "p_trainloader_ae = DataLoader(p_trainset, batch_size=batch_ae, shuffle=True)\n",
        "X_poison_np = X_poison.cpu().numpy()\n",
        "y_poison_np = y_poison.cpu().numpy()\n",
        "X_valid_np = X_valid.cpu().numpy()\n",
        "y_valid_np = y_valid.cpu().numpy()\n",
        "p_X_train_np = p_X_train.cpu().numpy()\n",
        "p_y_train_np = p_y_train.cpu().numpy()\n",
        "\n",
        "# parameters for the clean model\n",
        "if netType==1:\n",
        "    c_model = lgb.LGBMClassifier()\n",
        "    c_model.fit(p_X_train_np, p_y_train_np)\n",
        "    _, acc, fnr, fpr = test_gbm(c_model, X_test_np, y_test_np)\n",
        "else:\n",
        "    c_model = LightNN(X_train.shape[1]).to(device=device)\n",
        "    c_optimizer = torch.optim.SGD(c_model.parameters(), lr=lr_lightNN)\n",
        "    criterion = nn.MSELoss()\n",
        "    train(epochs_lightNN, c_model, c_optimizer, p_trainloader, X_test, y_test)\n",
        "    _, acc, fnr, fpr = test(c_model, X_test, y_test)\n",
        "print(\"p\",p, \"N\",N)\n",
        "print(\"Accuracy: {:3.3}[%] \".format(100 * acc))\n",
        "print(\"False-negative rate: {:3.3}[%]\".format(100 * fnr))\n",
        "\n",
        "if netType==1:\n",
        "    _, acc, fnr, fpr = test_gbm(c_model, X_poison_np, y_poison_np)\n",
        "    print(\"False-positive rate: {:3.3}[%]\".format(100 * fpr))\n",
        "else:\n",
        "    _, acc, fnr, fpr = test(c_model, p_X_train, p_y_train)\n",
        "    print(\"False-positive rate: {:3.3}[%]\".format(100 * fpr))\n",
        "\n",
        "\n",
        "# attack success rate\n",
        "if netType==1:\n",
        "    _, acc, attack, fpr = test_gbm(c_model, X_valid_np, y_valid_np)\n",
        "else:\n",
        "    _, acc, attack, fpr = test(c_model, X_valid, y_valid)\n",
        "print(\"Attack Success Rate (mal -> good) : {:3.4}[%]\".format(100 * attack))\n",
        "\n",
        "input_size = N2\n",
        "print(N2)\n",
        "output_size = input_size\n",
        "auto_encoder = AutoEncoder(input_size).to(device=device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_ae = torch.optim.SGD(auto_encoder.parameters(), lr=lr_ae) # 0.1\n",
        "\n",
        "print(\"Training AE...\")\n",
        "h_loss = train_ae(auto_encoder, criterion, optimizer_ae, epochs_ae, p_trainloader_ae, F_defender)\n",
        "\n",
        "nEnc = 512\n",
        "if encOnly == True:\n",
        "    ae_p_X_train = torch.zeros(p_X_train.shape[0], 512+nEnc).to(device=device)  # initialize \n",
        "    ae_p_X_train[:,:512] =  p_X_train[:,:512] \n",
        "    ae_p_X_train[:,512:] = auto_encoder.encode(p_X_train[:,F_defender]).clone().detach()\n",
        "else:\n",
        "    ae_p_X_train = p_X_train.clone().detach()\n",
        "    ae_p_X_train[:,F_defender] = auto_encoder(p_X_train[:,F_defender]).clone().detach()\n",
        "\n",
        "ae_p_trainset = DataSet(ae_p_X_train, p_y_train)\n",
        "ae_p_trainloader = DataLoader(ae_p_trainset, batch_size=batch_reducedNN, shuffle=True)\n",
        "\n",
        "if encOnly == True:\n",
        "    X_test_ae = torch.zeros(X_test.shape[0], 512+nEnc).to(device=device)  # initialize \n",
        "    X_valid_ae = torch.zeros(X_valid.shape[0], 512+nEnc).to(device=device)  # initialize \n",
        "    X_test_ae[:,:512] =  X_test[:,:512] \n",
        "    X_valid_ae[:,:512] =  X_valid[:,:512] \n",
        "    X_test_ae[:,512:] = auto_encoder.encode(X_test[:,F_defender]).clone().detach()\n",
        "    X_valid_ae[:,512:] = auto_encoder.encode(X_valid[:,F_defender]).clone().detach()\n",
        "else:\n",
        "    X_test_ae = X_test.detach().clone().to(device=device) \n",
        "    X_valid_ae = X_valid.detach().clone().to(device=device) \n",
        "    X_test_ae[:,F_defender] = auto_encoder(X_test[:,F_defender]).detach().clone().to(device=device)  # replace test data by fake data\n",
        "    X_valid_ae[:,F_defender] = auto_encoder(X_valid[:,F_defender]).detach().clone().to(device=device)  # replace validation data by fake data\n",
        "\n",
        "y_train_ae = p_y_train.to(device=device)\n",
        "y_test_ae = y_test.to(device=device)\n",
        "y_valid_ae = y_valid.to(device=device)\n",
        "ae_X_train_np  = ae_p_X_train.cpu().numpy()\n",
        "ae_y_train_np  = y_train_ae.cpu().numpy()\n",
        "ae_X_test_np  = X_test_ae.cpu().numpy()\n",
        "ae_y_test_np  = y_test_ae.cpu().numpy()\n",
        "ae_X_valid_np  = X_valid_ae.cpu().numpy()\n",
        "ae_y_valid_np  = y_valid_ae.cpu().numpy()\n",
        "\n",
        "if netType ==1:\n",
        "    p_model = lgb.LGBMClassifier()\n",
        "    p_model.fit(ae_X_train_np, ae_y_train_np)\n",
        "    _, acc, attack, fpr = test_gbm(p_model, ae_X_test_np, ae_y_test_np)\n",
        "    print(\"p\",p, \"N\",N)\n",
        "    print(\"Accuracy: {:3.3}[%] \".format(100 * acc))\n",
        "    print(\"False-negative rate: {:3.3}[%]\".format(100 * fnr))\n",
        "else:\n",
        "    p_model = reducedNN(ae_p_X_train.shape[1]).to(device=device)\n",
        "    p_optimizer = torch.optim.SGD(p_model.parameters(), lr=lr_reducedNN)\n",
        "    train(epochs_reducedNN, p_model, p_optimizer, ae_p_trainloader, X_test_ae, y_test_ae)\n",
        "    _, acc, fnr, fpr = test(p_model, X_test_ae, y_test_ae)\n",
        "    print(\"p\",p, \"N\",N)\n",
        "    print(\"Accuracy: {:3.3}[%] FN:{:3.3}[%]  FP: {:3.3}[%]\".format(100 * acc,100 * fnr,100 * fpr))\n",
        "\n",
        "\n",
        "if netType ==1:\n",
        "    _, acc, attack, fpr = test_gbm(p_model, ae_X_train_np[-p:,], ae_y_train_np[-p:])\n",
        "else:\n",
        "    _, acc, attack, fpr = test(p_model, ae_p_X_train[-p:,], p_y_train[-p:]) \n",
        "    \n",
        "if algType != 3:\n",
        "    print(\"False-positive rate: {:3.3}[%]\".format(100 * fpr))\n",
        "else:\n",
        "    print(\"False-negative rate: {:3.3}[%]\".format(100 * fnr))\n",
        "\n",
        "if netType ==1:\n",
        "    _, acc, attack, fpr = test_gbm(p_model, ae_X_valid_np, ae_y_valid_np)\n",
        "else:\n",
        "    _, acc, attack, fpr = test(p_model, X_valid_ae, y_valid_ae)\n",
        "print(\"Attack Success Rate : {:3.4}[%]\".format(100 * attack))\n",
        "\n",
        "del model\n",
        "del auto_encoder\n",
        "del p_model\n",
        "\n",
        "if netType !=1:\n",
        "    del optimizer\n",
        "    del optimizer_ae \n",
        "    del p_optimizer    \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Backdoor : 8 / 2351 (0.34 [%])\n",
            "# of Poison   : 80 / 8000 (1.0 [%])\n",
            "Epoch:10/100, Avg loss:0.004764 Test loss:0.03061 Test accuracy:0.962 FPR:0.0362\n",
            "Epoch:20/100, Avg loss:0.001944 Test loss:0.02923 Test accuracy:0.963 FPR:0.0382\n",
            "Epoch:30/100, Avg loss:0.001416 Test loss:0.02889 Test accuracy:0.961 FPR:0.0421\n",
            "Epoch:40/100, Avg loss:0.001039 Test loss:0.02724 Test accuracy:0.968 FPR:0.0313\n",
            "Epoch:50/100, Avg loss:0.0009906 Test loss:0.02609 Test accuracy:0.965 FPR:0.0333\n",
            "Epoch:60/100, Avg loss:0.0008022 Test loss:0.02563 Test accuracy:0.969 FPR:0.0304\n",
            "Epoch:70/100, Avg loss:0.0006993 Test loss:0.02878 Test accuracy:0.963 FPR:0.0372\n",
            "Epoch:80/100, Avg loss:0.0007075 Test loss:0.02927 Test accuracy:0.962 FPR:0.0362\n",
            "Epoch:90/100, Avg loss:0.0005552 Test loss:0.03093 Test accuracy:0.963 FPR:0.0353\n",
            "Epoch:100/100, Avg loss:0.0007595 Test loss:0.03089 Test accuracy:0.959 FPR:0.0421\n",
            "Accuracy: 96.4[%] FN:4.39[%]  FP: 2.94[%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top N feature indices F: [637, 642, 618, 621, 665, 692, 658, 660]\n",
            "Top N feature values V: [-1.          1.          1.         -1.         -1.         -0.29411763\n",
            " -0.33333331 -1.        ]\n",
            "Epoch:10/100, Avg loss:0.004745 Test loss:0.03121 Test accuracy:0.961 FPR:0.0402\n",
            "Epoch:20/100, Avg loss:0.001968 Test loss:0.02899 Test accuracy:0.962 FPR:0.0382\n",
            "Epoch:30/100, Avg loss:0.002061 Test loss:0.03052 Test accuracy:0.964 FPR:0.0323\n",
            "Epoch:40/100, Avg loss:0.001309 Test loss:0.0299 Test accuracy:0.961 FPR:0.0402\n",
            "Epoch:50/100, Avg loss:0.0009856 Test loss:0.03074 Test accuracy:0.964 FPR:0.0362\n",
            "Epoch:60/100, Avg loss:0.0008016 Test loss:0.03021 Test accuracy:0.966 FPR:0.0333\n",
            "Epoch:70/100, Avg loss:0.0007547 Test loss:0.02989 Test accuracy:0.961 FPR:0.0353\n",
            "Epoch:80/100, Avg loss:0.0007187 Test loss:0.0308 Test accuracy:0.96 FPR:0.0372\n",
            "Epoch:90/100, Avg loss:0.0007962 Test loss:0.02976 Test accuracy:0.964 FPR:0.0313\n",
            "Epoch:100/100, Avg loss:0.0004195 Test loss:0.02935 Test accuracy:0.964 FPR:0.0372\n",
            "p 80 N 8\n",
            "Accuracy: 96.6[%] \n",
            "False-negative rate: 3.98[%]\n",
            "False-positive rate: 0.0739[%]\n",
            "Attack Success Rate (mal -> good) : 63.33[%]\n",
            "1839\n",
            "Training AE...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:10/50, Train loss:0.0178\n",
            "Epoch:20/50, Train loss:0.0135\n",
            "Epoch:30/50, Train loss:0.0132\n",
            "Epoch:40/50, Train loss:0.0131\n",
            "Epoch:50/50, Train loss:0.0131\n",
            "Epoch:10/200, Avg loss:0.07089 Test loss:0.09548 Test accuracy:0.875 FPR:0.168\n",
            "Epoch:20/200, Avg loss:0.04836 Test loss:0.07559 Test accuracy:0.892 FPR:0.142\n",
            "Epoch:30/200, Avg loss:0.03584 Test loss:0.06638 Test accuracy:0.915 FPR:0.0911\n",
            "Epoch:40/200, Avg loss:0.02645 Test loss:0.06148 Test accuracy:0.919 FPR:0.0872\n",
            "Epoch:50/200, Avg loss:0.02136 Test loss:0.06019 Test accuracy:0.921 FPR:0.095\n",
            "Epoch:60/200, Avg loss:0.01917 Test loss:0.05856 Test accuracy:0.924 FPR:0.0803\n",
            "Epoch:70/200, Avg loss:0.0145 Test loss:0.05419 Test accuracy:0.932 FPR:0.0558\n",
            "Epoch:80/200, Avg loss:0.01474 Test loss:0.06085 Test accuracy:0.919 FPR:0.101\n",
            "Epoch:90/200, Avg loss:0.01018 Test loss:0.05693 Test accuracy:0.924 FPR:0.0842\n",
            "Epoch:100/200, Avg loss:0.01231 Test loss:0.05103 Test accuracy:0.934 FPR:0.0588\n",
            "Epoch:110/200, Avg loss:0.01028 Test loss:0.04865 Test accuracy:0.938 FPR:0.049\n",
            "Epoch:120/200, Avg loss:0.007914 Test loss:0.04798 Test accuracy:0.94 FPR:0.0529\n",
            "Epoch:130/200, Avg loss:0.009232 Test loss:0.04785 Test accuracy:0.939 FPR:0.0539\n",
            "Epoch:140/200, Avg loss:0.008021 Test loss:0.05063 Test accuracy:0.937 FPR:0.046\n",
            "Epoch:150/200, Avg loss:0.008161 Test loss:0.04902 Test accuracy:0.943 FPR:0.048\n",
            "Epoch:160/200, Avg loss:0.005469 Test loss:0.0507 Test accuracy:0.933 FPR:0.0666\n",
            "Epoch:170/200, Avg loss:0.00672 Test loss:0.04758 Test accuracy:0.936 FPR:0.0529\n",
            "Epoch:180/200, Avg loss:0.007405 Test loss:0.04638 Test accuracy:0.94 FPR:0.0529\n",
            "Epoch:190/200, Avg loss:0.006314 Test loss:0.04679 Test accuracy:0.94 FPR:0.0607\n",
            "Epoch:200/200, Avg loss:0.007297 Test loss:0.04656 Test accuracy:0.942 FPR:0.049\n",
            "p 80 N 8\n",
            "Accuracy: 94.8[%] FN:6.13[%]  FP: 4.21[%]\n",
            "False-positive rate: 0.0[%]\n",
            "Attack Success Rate : 7.865[%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "invalid value encountered in double_scalars\n"
          ]
        }
      ]
    }
  ]
}